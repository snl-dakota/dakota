#@ s*: Label=FastTest
#@ Allow default serial test: #s0
#@ s0: DakotaConfig=HAVE_DOT
#@ s1: DakotaConfig=HAVE_DOT
#@ s2: DakotaConfig=HAVE_DOT
#@ s3: DakotaConfig=HAVE_DOT
#@ p0: DakotaConfig=HAVE_DOT
#@ p1: DakotaConfig=HAVE_DOT
#@ p2: DakotaConfig=HAVE_DOT
#@ p3: DakotaConfig=HAVE_DOT
#@ p4: DakotaConfig=HAVE_DOT
#@ s4: DakotaConfig=HAVE_ROL
#@ p5: DakotaConfig=HAVE_ROL
#@ p6: DakotaConfig=HAVE_ROL
#@ p0: MPIProcs=3 CheckOutput='dakota.out.1'
#@ p1: MPIProcs=3
#@ p2: MPIProcs=4 CheckOutput='dakota.out.1'
#@ p3: MPIProcs=4
#@ p4: MPIProcs=3
#@ p5: MPIProcs=4 CheckOutput='dakota.out.1'
#@ p6: MPIProcs=3
#@ s0: UserMan=qsf_multistart_strat
#@ [taxonomy:start]
#@ s0: [analysis:Optimization]
#@ s0: [method:Newton]
#@ s0: [goal:Local]
#@ s0: [goal:BoundConstraints]
#@ s0: [variable:Continuous]
#@ s0: [model:Smooth]
#@ s0: [model:FirstDerivatives]
#@ [taxonomy:end]
#
#Test s4 uses ROL instead of NLP

# DAKOTA INPUT FILE - dakota_multistart.in
# Dakota Input File: qsf_multistart_strat.in                  #s0

# Demonstrates the use of the multi_start environment for a
# multimodal test problem.  The global optimum is at the
# point (x1,x2) = (0.177,0.177) which has a function value
# of 0.060.

# Parallel test matrix is dedicated scheduler and peer partition
# scheduling for concurrent iterators with single-processor and
# multiprocessor iterator partitions. Parallel tests 2 and 3 include
# an idle partition due to over-allocation

environment
  top_method_pointer = 'MS'

method
  id_method = 'MS'
  multi_start
#    iterator_servers = 3          #p0,#p2,#p5
#    iterator_scheduling peer      #p0,#p2,#p5
#    iterator_servers = 2          #p1,#p3,#p4,#p6
#    iterator_scheduling dedicated #p1,#p3,#p4,#p6
#    processors_per_iterator = 1   #p2,#p3,#p5
    method_pointer = 'NLP'         #s0,#s1,#s2,#s4,#p0,#p1,#p2,#p3,#p5,#p6
#    method_name 'dot_bfgs'        #s3,#p4
    random_starts = 3 seed = 123
    starting_points = -0.8  -0.8
                      -0.8   0.8
                       0.8  -0.8
                       0.8   0.8
                       0.0   0.0

method                            #s0,#s1,#s2,#s4,#p0,#p1,#p2,#p3,#p4,#p5,#p6
  id_method = 'NLP'               #s0,#s1,#s2,#s4,#p0,#p1,#p2,#p3,#p4,#p5,#p6
## (DOT requires a software license; if not available, try	      #s0
## conmin_mfd or optpp_q_newton instead)     			      #s0
  dot_bfgs                        #s0,#s1,#s2,#p0,#p1,#p2,#p3,#p4
#   scaling                       #s1,#s2
#  rol                               #s4,#p5,#p6
#    gradient_tolerance 1.0e-12         #s4,#p5,#p6
#    constraint_tolerance 1.0e-12       #s4,#p5,#p6
#    variable_tolerance 1.0e-12            #s4,#p5,#p6

variables
  continuous_design = 2
    lower_bounds    -1.0     -1.0
    upper_bounds     1.0      1.0
    descriptors      'x1'     'x2'
# Test scaling with multi-start
#   scale_types   'value'   'auto'  #s1
#   scales          10.0      1.0   #s1
# Test scaling with multi-start: verify non-scaling works as expected
#   scale_types   'value'   'value'  #s2
#   scales           1.0       1.0   #s2


interface
  analysis_drivers = 'quasi_sine_fcn'
    fork #asynchronous

responses
  objective_functions = 1
  analytic_gradients
  no_hessians
