namespace Dakota {

/** \page IntroCommands Introduction

\htmlonly
<b>Commands Introduction Table of Contents</b>
<ul>
<li> <a href="IntroCommands.html#IntroCmdsOverview">Overview</a>
<li> <a href="IntroCommands.html#IntroCmdsInpSpec">
     NIDR Input Specification File</a>
<li> <a href="IntroCommands.html#IntroCmdsMistakes">
     Common Specification Mistakes</a>
<li> <a href="IntroCommands.html#IntroCmdsSamples">Sample dakota.in Files</a>
  <ul>
  <li> <a href="IntroCommands.html#IntroCmdsSampleOpt">
       Sample 1: Optimization</a>
  <li> <a href="IntroCommands.html#IntroCmdsSampleLS">
       Sample 2: Least Squares (Calibration)</a>
  <li> <a href="IntroCommands.html#IntroCmdsSampleUQ">
       Sample 3: Nondeterministic Analysis</a>
  <li> <a href="IntroCommands.html#IntroCmdsSamplePS">
       Sample 4: Parameter Study</a>
  <li> <a href="IntroCommands.html#IntroCmdsSampleML">
       Sample 5: Hybrid Strategy</a>
  </ul>
<li> <a href="IntroCommands.html#IntroCmdsTables">Tabular descriptions</a>
</ul>
\endhtmlonly


\section IntroCmdsOverview Overview


In DAKOTA, a \e strategy creates and manages \e iterators and \e
models. A model, generally speaking, contains a set of \e variables,
an \e interface, and a set of \e responses.  An iterator repeatedly
operates on the model to map the variables into responses using the
interface. Each of these six pieces (strategy, method, model,
variables, interface, and responses) are separate specifications in
the user's input file, and as a whole, determine the study to be
performed during an execution of the DAKOTA software. A DAKOTA
execution is limited to a single strategy, which may invoke multiple
methods. Furthermore, each method may have its own model, consisting
of (generally speaking) its own variables, interface, and set of
responses. Thus, there may be multiple specifications of the method,
model, variables, interface, and responses sections.

DAKOTA input is most commonly specified through a text file, whose
syntax is governed by the New Input Deck Reader (NIDR) parsing system
[\ref Gay2008 "Gay, 2008"], in conjunction with the dakota.input.nspec
file which describes allowable inputs.  A more concise version of the
input specification file, dakota.input.summary, offers a quick
reference to keywords and data values from which a particular input
file (e.g., \c dakota.in) may be derived.  This automatically derived
shortened form omits implementation details not needed in a quick
reference.

This Reference Manual focuses on the details of allowable
specifications in a file input to the DAKOTA program.  Related details
on the name and location of the \c dakota program, command line
inputs, and execution syntax are provided in the Users Manual [\ref
UsersMan "Adams et al., 2010"].


\section IntroCmdsInpSpec NIDR Input Specification File


DAKOTA input is governed by the NIDR input specification file. This
file (dakota.input.nspec) is used by a code generator to create
parsing system components that are compiled into the DAKOTA executable
(refer to \ref SpecChange for additional information). Therefore,
dakota.input.nspec and its derived summary, dakota.input.summary, are
the definitive source for input syntax, capability options, and
optional and required capability sub-parameters for any given DAKOTA
version. Beginning users may find dakota.input.summary overwhelming or
confusing and will likely derive more benefit from adapting example
input files to a particular problem. However, advanced users can
master the many input specification possibilities by understanding the
structure of the input specification file.

Refer to the dakota.input.summary file for current input
specifications.  From this file listing, it can be seen that the main
structure of the strategy specification consists several required
group specifications separated by logical OR's (indicated by \c |):
either hybrid OR multi-start OR pareto set OR single method.  The
method keyword is the most lengthy specification; however, its
structure is again relatively simple. The structure is a set of
optional method-independent settings followed by a long list of
possible methods appearing as required group specifications
(containing a variety of method-dependent settings) separated by OR's.
The model keyword reflects a structure of three required group
specifications separated by OR's.  Within the surrogate model type,
the type of approximation must be specified with either a global OR
multipoint OR local OR hierarchical required group specification.  The
structure of the variables keyword is that of optional group
specifications for continuous and discrete design variables, a number
of different uncertain variable distribution types, and continuous and
discrete state variables. Each of these specifications can either
appear or not appear as a group. Next, the interface keyword allows
the specification of either algebraic mappings, simulation-based
analysis driver mappings, or both.  Within the analysis drivers
specification, a system OR fork OR direct OR grid group specification
must be selected.  Finally, within the responses keyword, the primary
structure is the required specification of the function set (either
optimization functions OR calibration functions OR generic response
functions), followed by the required specification of the gradients
(either none OR numerical OR analytic OR mixed) and the required
specification of the Hessians (either none OR numerical OR quasi OR
analytic OR mixed).  Refer to \ref StratCommands, \ref MethodCommands,
\ref ModelCommands, \ref VarCommands, \ref InterfCommands, and \ref
RespCommands for detailed information on the keywords and their
various optional and required specifications. And for additional
details on NIDR specification logic and rules, refer to [\ref Gay2008
"Gay, 2008"].

Some keywords, such as those providing bounds on variables, have an
associated list of values.  When the same value should be repeated
several times in a row, you can use the notation \c N*value instead of
repeating the value N times.  For example, in \ref IntroCmdsSampleLS
below,
\verbatim
          lower_bounds    -2.0   -2.0
          upper_bounds     2.0    2.0
\endverbatim
could also be written
\verbatim
          lower_bounds    2*-2.0
          upper_bounds    2 * 2.0
\endverbatim
(with optional spaces around the \c * ).  Another possible abbreviation
is for sequences:  L:S:U (with optional spaces around the : )
is expanded to L L+S L+2*S ... U, and L:U (with no second colon) is
treated as L:1:U.  For example, in one of the test examples distributed
with DAKOTA (test case 2 of \c test/dakota_uq_textbook_sop_lhs.in ),
\verbatim
        histogram_point = 2                             
          abscissas     = 50. 60. 70. 80. 90.           
                          30. 40. 50. 60. 70.           
          counts        = 10  20  30  20  10            
                          10  20  30  20  10            
\endverbatim
could also be written
\verbatim
        histogram_point = 2                             
          abscissas     = 50 : 10 : 90
                          30 : 10 : 70                  
          counts        = 10:10:30  20  10              
                          10:10:30  20  10              
\endverbatim

Count and sequence abbreviations can be used together.  For example
\verbatim
response_levels =
	0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
	0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
\endverbatim
can be abbreviated 
\verbatim
response_levels =
	2*0.0:0.1:1.0
\endverbatim

\section IntroCmdsMistakes Common Specification Mistakes


Spelling mistakes and omission of required parameters are the most
common errors.  Some causes of errors are more obscure:

\li Documentation of new capability sometimes lags its availability in
source and executables, especially nightly releases. When parsing
errors occur that the documentation cannot explain, reference to the
particular input specification used in building the executable, which
is installed alongside the executable, will often resolve the errors.

\li If you want to compare results with those obtained using
an earlier version of DAKOTA (prior to 4.1), your input file
for the earlier version must use backslashes to indicate
continuation lines for DAKOTA keywords.  For example, rather than
\verbatim
# Comment about the following "responses" keyword...
responses,
        objective_functions = 1
        # Comment within keyword "responses"
        analytic_gradients
# Another comment within keyword "responses"
        no_hessians
\endverbatim
you would need to write
\verbatim
# Comment about the following "responses" keyword...
responses,                                              \
        objective_functions = 1                     \
        # Comment within keyword "responses"            \
        analytic_gradients                              \
# Another comment within keyword "responses"            \
        no_hessians
\endverbatim
with no white space (blanks or tabs) after the \ character.

In most cases, the NIDR system provides error messages that
help the user isolate errors in DAKOTA input files.


\section IntroCmdsSamples Sample dakota.in Files


A DAKOTA input file is a collection of fields from the
dakota.input.summary file that describe the problem to be solved by
DAKOTA. Several examples follow.


\subsection IntroCmdsSampleOpt Sample 1: Optimization

The following sample input file shows single-method optimization of
the Textbook Example using DOT's modified method of feasible
directions. A similar file is available as \c
Dakota/examples/users/textbook_opt_conmin.in.

<!-- \include dakota_textbook.in -->

\verbatim
strategy,
        single_method

method,
#       DOT performs better, but may not be available
        dot_mmfd,
#       conmin_mfd,
          max_iterations = 50,
          convergence_tolerance = 1e-4

variables,
        continuous_design = 2
          initial_point    0.9    1.1
          upper_bounds     5.8    2.9
          lower_bounds     0.5   -2.9
          descriptors      'x1'   'x2'

interface,
        direct
          analysis_driver =       'text_book'

responses,
        objective_functions = 1
        nonlinear_inequality_constraints = 2
        numerical_gradients
          method_source dakota
          interval_type central
          fd_gradient_step_size = 1.e-4
        no_hessians
\endverbatim


\subsection IntroCmdsSampleLS Sample 2: Least Squares (Calibration)

The following sample input file shows a nonlinear least squares
(calibration) solution of the Rosenbrock Example using the NL2SOL
method. A similar file is available as \c
Dakota/examples/users/rosen_opt_nls.in

<!-- \include dakota_rosenbrock_ls.in -->

\verbatim
strategy,
        single_method

method,
        nl2sol
          max_iterations = 50
          convergence_tolerance = 1e-4

model,
        single

variables,
        continuous_design = 2
          initial_point   -1.2    1.0
          lower_bounds    -2.0   -2.0
          upper_bounds     2.0    2.0
          descriptor       'x1'   'x2'

interface,
        fork
          analysis_driver = 'rosenbrock'

responses,
        calibration_terms = 2
        analytic_gradients
        no_hessians
\endverbatim


\subsection IntroCmdsSampleUQ Sample 3: Nondeterministic Analysis

The following sample input file shows Latin Hypercube Monte Carlo
sampling using the Textbook Example. A similar file is available as \c
Dakota/test/dakota_uq_textbook_lhs.in.

<!-- \include dakota_textbook_lhs.in -->

\verbatim
strategy,
        single_method

method,
        sampling,
          samples = 100 seed = 1
          complementary distribution
          response_levels = 3.6e+11 4.0e+11 4.4e+11
                            6.0e+04 6.5e+04 7.0e+04
                            3.5e+05 4.0e+05 4.5e+05
          sample_type lhs

variables,
        normal_uncertain = 2
          means             =  248.89, 593.33
          std_deviations    =   12.4,   29.7
          descriptors       =  'TF1n'  'TF2n'
        uniform_uncertain = 2
          lower_bounds      =  199.3,  474.63
          upper_bounds      =  298.5,  712.
          descriptors       =  'TF1u'  'TF2u'
        weibull_uncertain = 2
          alphas            =   12.,    30.
          betas             =  250.,   590.
          descriptors       =  'TF1w'  'TF2w'
        histogram_bin_uncertain = 2
          num_pairs   =  3         4
          abscissas   =  5  8 10  .1  .2  .3  .4
          counts      = 17 21  0  12  24  12   0
          descriptors = 'TF1h'  'TF2h'
        histogram_point_uncertain = 1
          num_pairs   = 2
          abscissas   = 3 4
          counts      = 1 1
          descriptors = 'TF3h'

interface,
        fork asynch evaluation_concurrency = 5
          analysis_driver = 'text_book'

responses,
        response_functions = 3
        no_gradients
        no_hessians
\endverbatim


\subsection IntroCmdsSamplePS Sample 4: Parameter Study

The following sample input file shows a 1-D vector parameter study
using the Textbook Example. It makes use of the default strategy and
model specifications (\c single_method and \c single, respectively),
so they can be omitted.  A similar file is available in the test
directory as \c Dakota/examples/users/rosen_ps_vector.in.

<!-- \include dakota_pstudy.in -->

\verbatim
strategy,
        single_method

method,
        vector_parameter_study
          final_point = 1.1  1.3
          num_steps = 10

model,
        single

variables,
        continuous_design = 2
          initial_point   -0.3      0.2
          descriptors      'x1'     "x2"

interface,
        direct
          analysis_driver = 'rosenbrock'

responses,
        objective_functions = 1
        no_gradients
        no_hessians
\endverbatim


\subsection IntroCmdsSampleML Sample 5: Hybrid Strategy

The following sample input file shows a hybrid strategy using three
methods. It employs a genetic algorithm, pattern search, and full
Newton gradient-based optimization in succession to solve the Textbook
Example. A similar file is available as \c
Dakota/examples/users/textbook_hybrid_strat.in.

<!-- \include dakota_hybrid.in -->

\verbatim
strategy,
        graphics
        hybrid sequential
          method_list = 'GA' 'PS' 'NLP'

method,
        id_method = 'GA'
        model_pointer = 'M1'
        coliny_ea
          seed = 1234
          population_size = 10
          verbose output

method,
        id_method = 'PS'
        model_pointer = 'M1'
        coliny_pattern_search stochastic
          seed = 1234
          initial_delta = 0.1
          threshold_delta = 1.e-4
          solution_accuracy = 1.e-10
          exploratory_moves basic_pattern
          verbose output

method,
        id_method = 'PS2'
        model_pointer = 'M1'
        max_function_evaluations = 10
        coliny_pattern_search stochastic
          seed = 1234
          initial_delta = 0.1
          threshold_delta = 1.e-4
          solution_accuracy = 1.e-10
          exploratory_moves basic_pattern
          verbose output

method,
        id_method = 'NLP'
        model_pointer = 'M2'
        optpp_newton
          gradient_tolerance = 1.e-12
          convergence_tolerance = 1.e-15
          verbose output

model,
        id_model = 'M1'
        single
          variables_pointer = 'V1'
          interface_pointer = 'I1'
          responses_pointer = 'R1'

model,
        id_model = 'M2'
        single
          variables_pointer = 'V1'
          interface_pointer = 'I1'
          responses_pointer = 'R2'

variables,
        id_variables = 'V1'
        continuous_design = 2
          initial_point    0.6    0.7
          upper_bounds     5.8    2.9
          lower_bounds     0.5   -2.9
          descriptors      'x1'   'x2'

interface,
        id_interface = 'I1'
        direct
          analysis_driver=  'text_book'

responses,
        id_responses = 'R1'
        objective_functions = 1
        no_gradients
        no_hessians

responses,
        id_responses = 'R2'
        objective_functions = 1
        analytic_gradients
        analytic_hessians
 \endverbatim

Additional example input files, as well as the corresponding output
and graphics, are provided in the Tutorial chapter of the Users Manual
[\ref UsersMan "Adams et al., 2010"].


\section IntroCmdsTables Tabular descriptions


In the following discussion of keyword specifications, tabular formats
(Tables \ref T4d1 "4.1" through \ref T9d11 "9.11") are used to present
a short description of the specification, the keyword used in the
specification, the type of data associated with the keyword, the
status of the specification (required, optional, required group, or
optional group), and the default for an optional specification.

It can be difficult to capture in a simple tabular format the complex
relationships that can occur when specifications are nested within
multiple groupings. For example, in the model keyword, the \c
actual_model_pointer specification is a required specification within
the \c multipoint and \c local required group specifications, which
are separated from each other and from other required group
specifications (\c global and \c hierarchical) by logical OR's. The
selection between the \c global, \c multipoint, \c local, or \c
hierarchical required groups is contained within another required
group specification (\c surrogate), which is separated from the \c
single and \c nested required group specifications by logical
OR's. Rather than unnecessarily proliferate the number of tables in
attempting to capture all of these inter-relationships, a balance is
sought, since some inter-relationships are more easily discussed in
the associated text.  The general structure of the following sections
is to present the outermost specification groups first (e.g., \c single,
\c surrogate, or \c nested in Table \ref T6d1 "6.1"), followed
by lower levels of group specifications (e.g., \c global, \c multipoint,
\c local, or \c hierarchical surrogates in Table \ref T6d3 "6.3"),
followed by the components of each group (e.g., Tables \ref T6d4 "6.4"
through \ref T6d9 "6.9") in succession.

\htmlonly
<hr>
<br><b><a href="StratCommands.html#StratCommands">Next chapter</a></b>
\endhtmlonly

*/

/** \file dakota.input.summary
    \brief File containing the input specification for DAKOTA

This file is derived automatically from dakota.input.nspec, which is
used in the generation of parser system files that are compiled into
the DAKOTA executable. Therefore, these files are the definitive
source for input syntax, capability options, and associated data
inputs. Refer to \ref SpecChange for information on how to modify the
input specification and propagate the changes through the parsing
system.

Key features of the input specification and the associated user
input files include:

\li In the input specification, required individual specifications simply
appear, optional individual and group specifications are enclosed in
\c [], required group specifications are enclosed in \c (), and either-or
relationships are denoted by the \c | symbol. These symbols only
appear in dakota.input.nspec; they must not appear in actual user input
files.

\li Keyword specifications (i.e., \c strategy, \c method, \c model, \c
variables, \c interface, and \c responses) begin with the keyword
possibly preceded by white space (blanks, tabs, and newlines)
both in the input specifications and in user input
files.  For readability, keyword specifications may be spread across
several lines.  Earlier versions of DAKOTA (prior to 4.1) required
a backslash character (\\) at the ends of intermediate lines of
a keyword.  While such backslashes are still accepted, they are
no longer required.

<!-- MSE: Out of date for NIDR
\li Each of the six keywords in the input specification begins with a
\verbatim
<KEYWORD = name>, <FUNCTION = handler_name>
\endverbatim
header which names the keyword and provides the binding to the keyword
handler within DAKOTA's problem description database. In a user
input file, only the name of the keyword appears (e.g., \c variables).
-->

\li Some of the keyword components within the input specification
indicate that the user must supply <tt>INTEGER</tt>, <tt>REAL</tt>,
<tt>STRING</tt>, <tt>INTEGERLIST</tt>, <tt>REALLIST</tt>, or
<tt>STRINGLIST</tt> data as part of the specification. In a user input
file, the \c "=" is optional, data in a <tt>LIST</tt> can be separated
by commas or whitespace, and the <tt>STRING</tt> data are enclosed in
single or double quotes (e.g., \c 'text_book' or "text_book").


\li In user input files, input is largely order-independent (except for
entries in lists of data), case insensitive, and white-space
insensitive. Although the order of input shown in the \ref
IntroCmdsSamples generally follows the order of options in the input
specification, this is not required.

\li In user input files, specifications may be abbreviated so long as
the abbreviation is unique. For example, the \c npsol_sqp
specification within the method keyword could be abbreviated as \c
npsol, but \c dot_sqp should not be abbreviated as \c dot since this
would be ambiguous with other DOT method specifications.

\li In both the input specification and user input files, comments are
preceded by \c #.

The dakota.input.summary file for DAKOTA version 5.1 is:

\include dakota.input.summary

*/

} // namespace Dakota
