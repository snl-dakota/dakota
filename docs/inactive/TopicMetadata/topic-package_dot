The DOT library \cite Van95 contains nonlinear programming optimizers,
specifically the Broyden-Fletcher-Goldfarb-Shanno (\ref
method-dot_bfgs) and Fletcher-Reeves conjugate gradient (\ref
method-dot_frcg) methods for unconstrained optimization, and the
modified method of feasible directions (\ref method-dot_mmfd),
sequential linear programming (\ref method-dot_slp), and sequential
quadratic programming (\ref method-dot_sqp) methods for constrained
optimization.  To use DOT, one of these methods must be specified.

Specialized handling of linear constraints is supported with DOT;
linear constraint coefficients, bounds, and targets can be provided to
DOT at start-up and tracked internally.

<b> Method Independent Controls - Stopping Critiera </b>

Stopping critiera are set by: \c max_iterations, \c
max_function_evaluations, \c convergence_tolerance, and \c
constraint_tolerance

Note: The \c convergence_tolerance criterion must be satisfied
    for two consecutive iterations before DOT will terminate. 

<b> Method Independent Controls - Output </b>

The output verbosity specification controls the amount of information
generated by DOT: the \c silent and \c quiet settings result in header
information, final results, and objective function, constraint, and
parameter information on each iteration; whereas the \c verbose and \c
debug settings add additional information on gradients, search
direction, one-dimensional search results, and parameter scaling
factors.

<b> Concurrency </b>

DOT contains no parallel algorithms which can directly take advantage
of concurrent evaluations. However, if \c numerical_gradients with \c
method_source \c dakota is specified, then the finite difference
function evaluations can be performed concurrently (using any of the
parallel modes described in the Users Manual \cite UsersMan).  In
addition, if \c speculative is specified, then gradients (\c dakota \c
numerical or \c analytic gradients) will be computed on each line
search evaluation in order to balance the load and lower the total run
time in parallel optimization studies.
