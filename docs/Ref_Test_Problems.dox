<!------ Test Problems, copied from Chap 20 --->

\page test_problems Test Problems

This page contains additional information about two test problems
that are used in Dakota examples throughout the Dakota manuals \subpage textbook and \subpage rosenbrock.

Many of these examples are also used as code verification tests. The examples are
run periodically and the results are checked against known solutions. This 
ensures that the algorithms are correctly implemented. 

Additional test problems are described in the User's Manual.

\page textbook Textbook

The two-variable version of the ``textbook'' test problem provides
a nonlinearly constrained optimization test case. It is formulated as:
\anchor textbookform
\f{align}{
\texttt{minimize }   & f = (x_1-1)^{4}+(x_2-1)^{4}     \nonumber \\
\texttt{subject to } & g_1 = x_1^2-\frac{x_2}{2} \le 0 \tag{textbookform} \\
                     & g_2 = x_2^2-\frac{x_1}{2} \le 0 \nonumber \\
                     & 0.5 \le x_1 \le 5.8             \nonumber \\
                     & -2.9 \le x_2 \le 2.9            \nonumber
\f}
<!----  NOTE THAT tags with underscores _ cause errors !!! ---->

Contours of this test problem are illustrated in the next two figures.

\image latex textbook_contours.eps "Contours of the textbook problem on the [-3,4] x [-3,4] domain. The feasible region lies at the intersection of the two constraints g_1 (solid) and g_2 (dashed)." height=2.5in
\image latex textbook_closeup.eps "Contours of the textbook problem zoomed into an area containing the constrained optimum point (x_1,x_2) = (0.5,0.5). The feasible region lies at the intersection of the two constraints g_1 (solid) and g_2 (dashed)." height=2.5in
\image html textbook_contours.png "Contours of the textbook problem on the [-3,4] x [-3,4] domain. The feasible region lies at the intersection of the two constraints g_1 (solid) and g_2 (dashed)."
\image html textbook_closeup.png "Contours of the textbook problem zoomed into an area containing the constrained optimum point (x_1,x_2) = (0.5,0.5). The feasible region lies at the intersection of the two constraints g_1 (solid) and g_2 (dashed)." 

For the textbook test problem, the unconstrained minimum occurs at
\f$(x_1,x_2) = (1,1)\f$. However, the inclusion of the constraints
moves the minimum to \f$(x_1,x_2) = (0.5,0.5)\f$.
Equation \ref textbookform  presents the 2-dimensional
form of the textbook problem. An extended formulation is stated as
\anchor tbe
\f{align}
\texttt{minimize }   & f = \sum_{i=1}^{n}(x_i-1)^4       \nonumber  \\
\texttt{subject to } & g_1 = x_1^2-\frac{x_2}{2} \leq 0  \tag{tbe}  \\
                     & g_2=x_2^2-\frac{x_1}{2} \leq 0    \nonumber  \\
                     & 0.5 \leq x_1 \leq 5.8             \nonumber  \\
                     & -2.9 \leq x_2 \leq 2.9            \nonumber
\f}
where \f$n\f$ is the number of design variables. The objective function is
designed to accommodate an arbitrary number of design variables in
order to allow flexible testing of a variety of data sets. Contour
plots for the \f$n=2\f$ case have been shown previously.

For the optimization problem given in Equation \ref tbe, the
unconstrained solution

(\c num_nonlinear_inequality_constraints
set to zero) for two design variables is:
\f{eqnarray*}
    x_1 &=& 1.0 \\
    x_2 &=& 1.0
\f}
with
\f{eqnarray*}
    f^{\ast} &=& 0.0
\f}

The solution for the optimization problem constrained by \f$g_1\f$\\
(\c num_nonlinear_inequality_constraints set to one) is:
\f{eqnarray*}
    x_1 &=& 0.763 \\
    x_2 &=& 1.16
\f}
with
\f{eqnarray*}
      f^{\ast} &=& 0.00388 \\
    g_1^{\ast} &=& 0.0 ~~\mathrm{(active)}
\f}

The solution for the optimization problem constrained by \f$g_1\f$ and \f$g_2\f$\\
(\c num_nonlinear_inequality_constraints set to two) is:
\f{eqnarray*}
    x_1 &=& 0.500 \\
    x_2 &=& 0.500
\f}
with
\f{eqnarray*}
      f^{\ast} &=& 0.125 \\
    g_1^{\ast} &=& 0.0 ~~\mathrm{(active)} \\
    g_2^{\ast} &=& 0.0 ~~\mathrm{(active)}
\f}

Note that as constraints are added, the design freedom is restricted
(the additional constraints are active at the solution) and an
increase in the optimal objective function is observed.

\page rosenbrock Rosenbrock

The Rosenbrock function \cite Gil81 is a well-known test problem
for optimization algorithms. The standard formulation includes two design variables,
and computes a single objective function.
This problem can also be posed as a least-squares optimization 
problem with two residuals to be minimzed because the objective 
function is the sum of squared terms.


<b> Standard Formulation </b>

The standard two-dimensional formulation can be stated as
\anchor rosenstd
\f{equation}
\texttt{minimize } f=100(x_2-x_1^2)^2+(1-x_1)^2 \tag{rosenstd}
\f}

Surface and contour plots for this function are shown in the Dakota
User's Manual.

The optimal solution is:
\f{eqnarray*}
    x_1 &=& 1.0 \\
    x_2 &=& 1.0
\f}
with
\f{eqnarray*}
    f^{\ast} &=& 0.0
\f}

<b> A Least-Squares Optimization Formulation </b>

This test problem
may also be used to exercise least-squares solution methods by
recasting the standard problem formulation into:
\anchor rosenls
\f{equation}
\texttt{minimize } f = (f_1)^2+(f_2)^2 \tag{rosenls}
\f}
where
\anchor rosenr1
\f{equation}
f_1 = 10 (x_2 - x_1^2) \tag{rosenr1}
\f}
and
\anchor rosenr2
\f{equation}
f_2 = 1 - x_1 \tag{rosenr2}
\f}
are residual terms.

The included analysis driver can handle both formulations.
In the \c Dakota/test directory, the \c rosenbrock
executable (compiled from \c Dakota_Source/test/rosenbrock.cpp) checks the number of
response functions passed in the parameters file and returns either an
objective function (as computed from
Equation \ref rosenstd) for use with optimization methods
or two least squares terms (as computed from
Equations \ref rosenr1 -\ref rosenr2 ) for use
with least squares methods. Both cases support analytic gradients of
the function set with respect to the design variables.
See the User's Manual for examples of both cases (search for Rosenbrock).

